import streamlit as st
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import joblib
import numpy as np
import requests
import json
from pathlib import Path

# å°è¯•å¯¼å…¥AIç›¸å…³æ¨¡å—ï¼Œå¦‚æœå¤±è´¥åˆ™æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
try:
    import google.generativeai as genai
    GOOGLE_AI_AVAILABLE = True
except ImportError as e:
    st.error(f"Google Generative AI å¯¼å…¥å¤±è´¥: {e}")
    st.error("è¯·ç¡®ä¿ google-generativeai åŒ…å·²æ­£ç¡®å®‰è£…")
    GOOGLE_AI_AVAILABLE = False
    genai = None

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError as e:
    st.error(f"OpenAI å¯¼å…¥å¤±è´¥: {e}")
    st.error("è¯·ç¡®ä¿ openai åŒ…å·²æ­£ç¡®å®‰è£…")
    OPENAI_AVAILABLE = False
    openai = None

# --- 0. å…¨å±€è®¾ç½®ä¸èµ„æºåŠ è½½ ---

st.set_page_config(
    page_title="å»ºç­‘æ€§èƒ½é¢„æµ‹ä¸æ™ºèƒ½å†³ç­–ç³»ç»Ÿ",
    page_icon="ğŸ¢",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- AIæœåŠ¡æä¾›å•†é…ç½® ---
st.sidebar.title("ğŸ¤– AIæœåŠ¡é…ç½®")

# AIæœåŠ¡æä¾›å•†é€‰æ‹©
ai_provider = st.sidebar.selectbox(
    "é€‰æ‹©AIæœåŠ¡æä¾›å•†",
    ["OpenAI", "Google Gemini", "DeepSeek"],
    help="é€‰æ‹©æ‚¨è¦ä½¿ç”¨çš„AIæœåŠ¡æä¾›å•†"
)

# æ˜¾ç¤ºå¯¹åº”çš„APIé…ç½®ä¿¡æ¯
if ai_provider == "OpenAI":
    if not OPENAI_AVAILABLE:
        st.sidebar.error("âŒ OpenAI æ¨¡å—æœªæ­£ç¡®å®‰è£…")
        AI_AVAILABLE = False
    else:
        st.sidebar.info("ğŸ”— è·å–APIå¯†é’¥: https://platform.openai.com/api-keys")
        api_key = st.sidebar.text_input("OpenAI API Key", type="password", help="è¾“å…¥æ‚¨çš„OpenAI APIå¯†é’¥")
        if api_key:
            openai.api_key = api_key
            AI_AVAILABLE = True
            st.sidebar.success("âœ… OpenAI API å·²é…ç½®")
        else:
            AI_AVAILABLE = False
            st.sidebar.warning("âš ï¸ è¯·è¾“å…¥OpenAI APIå¯†é’¥")
elif ai_provider == "Google Gemini":
    if not GOOGLE_AI_AVAILABLE:
        st.sidebar.error("âŒ Google Generative AI æ¨¡å—æœªæ­£ç¡®å®‰è£…")
        AI_AVAILABLE = False
    else:
        st.sidebar.info("ğŸ”— è·å–APIå¯†é’¥: https://aistudio.google.com/app/apikey")
        api_key = st.sidebar.text_input("Google API Key", type="password", help="è¾“å…¥æ‚¨çš„Google AI Studio APIå¯†é’¥")
        if api_key:
            genai.configure(api_key=api_key)
            AI_AVAILABLE = True
            st.sidebar.success("âœ… Google Gemini API å·²é…ç½®")
        else:
            AI_AVAILABLE = False
            st.sidebar.warning("âš ï¸ è¯·è¾“å…¥Google APIå¯†é’¥")
elif ai_provider == "DeepSeek":
    st.sidebar.info("ğŸ”— è·å–APIå¯†é’¥: https://platform.deepseek.com/api_keys")
    api_key = st.sidebar.text_input("DeepSeek API Key", type="password", help="è¾“å…¥æ‚¨çš„DeepSeek APIå¯†é’¥")
    if api_key:
        AI_AVAILABLE = True
        st.sidebar.success("âœ… DeepSeek API å·²é…ç½®")
    else:
        AI_AVAILABLE = False
        st.sidebar.warning("âš ï¸ è¯·è¾“å…¥DeepSeek APIå¯†é’¥")

st.sidebar.markdown("---")

# --- 1. ä¿®æ­£åçš„æ¨¡å‹å®šä¹‰ ---

# ä¸º UDI å’Œ DGI å®šä¹‰ä¸€ä¸ªæ¨¡å‹ç±»
# è¿™ä¸ªç»“æ„ä¸æ‚¨çš„ UDI_train.py å’Œ DGI_train.py å®Œå…¨ä¸€è‡´
class UDI_DGI_Model(nn.Module):
    def __init__(self, input_features=10):
        super().__init__()
        self.layer1 = nn.Linear(input_features, 256)  
        self.layer2 = nn.Linear(256, 512)
        self.layer3 = nn.Linear(512, 256)
        self.output_layer = nn.Linear(256, 1)

    def forward(self, x):
        x = torch.relu(self.layer1(x))
        x = torch.relu(self.layer2(x))
        x = torch.relu(self.layer3(x))
        x = self.output_layer(x)
        # ç§»é™¤ .squeeze() ä»¥ä¾¿ .item() å¯ä»¥ç›´æ¥ä½¿ç”¨
        return x

# ä¸º CEUI å®šä¹‰ä¸€ä¸ªç‹¬ç«‹çš„ã€æ›´å¤æ‚çš„æ¨¡å‹ç±»
# è¿™ä¸ªç»“æ„ä¸æ‚¨çš„ CEUI_train.py å®Œå…¨ä¸€è‡´
class CEUI_Model(nn.Module):
    def __init__(self, input_features=10):
        super().__init__()
        self.layer1 = nn.Linear(input_features, 256)
        self.bn1 = nn.BatchNorm1d(256)
        self.dropout1 = nn.Dropout(0.3)
        self.layer2 = nn.Linear(256, 512)
        self.bn2 = nn.BatchNorm1d(512)
        self.dropout2 = nn.Dropout(0.3)
        self.layer3 = nn.Linear(512, 256)
        self.bn3 = nn.BatchNorm1d(256)
        self.output_layer = nn.Linear(256, 1)

    def forward(self, x):
        # ç¡®ä¿åœ¨æ¨ç†æ—¶ï¼Œè¾“å…¥æ˜¯äºŒç»´çš„ [batch_size, features]
        # BatchNorm1d å±‚æœŸæœ›è‡³å°‘äºŒç»´çš„è¾“å…¥
        if x.dim() == 1:
            x = x.unsqueeze(0)
            
        x = self.dropout1(torch.relu(self.bn1(self.layer1(x))))
        x = self.dropout2(torch.relu(self.bn2(self.layer2(x))))
        x = torch.relu(self.bn3(self.layer3(x)))
        x = self.output_layer(x)
        # ç§»é™¤ .squeeze() ä»¥ä¾¿ .item() å¯ä»¥ç›´æ¥ä½¿ç”¨
        return x

@st.cache_resource
def load_models_and_scaler():
    """åŠ è½½æ‰€æœ‰PyTorchæ¨¡å‹å’Œæ•°æ®æ ‡å‡†åŒ–Scaler"""
    try:
        # ä½¿ç”¨æ­£ç¡®çš„ç±»æ¥å®ä¾‹åŒ–æ¯ä¸ªæ¨¡å‹
        udi_model = UDI_DGI_Model(input_features=10)
        dgi_model = UDI_DGI_Model(input_features=10)
        ceui_model = CEUI_Model(input_features=10)

        # åœ¨äº‘ç«¯éƒ¨ç½²æ—¶ï¼Œé€šå¸¸åªä½¿ç”¨CPU
        device = torch.device('cpu')
        st.sidebar.success(f"æ¨¡å‹å°†è¿è¡Œåœ¨: {str(device).upper()}")

        # åŠ è½½æ¨¡å‹æƒé‡ï¼Œå¹¶æ˜ å°„åˆ°CPU
        udi_model.load_state_dict(torch.load("UDI_trained_model.pth", map_location=device))
        dgi_model.load_state_dict(torch.load("DGI_trained_model.pth", map_location=device))
        ceui_model.load_state_dict(torch.load("CEUI_trained_model.pth", map_location=device))

        # å°†æ¨¡å‹æœ¬èº«ä¹Ÿç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
        udi_model.to(device)
        dgi_model.to(device)
        ceui_model.to(device)

        # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ (å¯¹äºæœ‰Dropoutå’ŒBatchNormçš„æ¨¡å‹è‡³å…³é‡è¦)
        udi_model.eval()
        dgi_model.eval()
        ceui_model.eval()

        # åŠ è½½Scalerå¯¹è±¡
        scaler = joblib.load('scaler.pkl') 
        
        return udi_model, dgi_model, ceui_model, scaler, device
    except FileNotFoundError as e:
        st.error(f"åŠ è½½æ–‡ä»¶æ—¶å‡ºé”™: {e}ã€‚è¯·ç¡®ä¿æ‰€æœ‰æ¨¡å‹æ–‡ä»¶ (UDI_trained_model.pth, DGI_trained_model.pth, CEUI_trained_model.pth) å’Œ scaler.pkl éƒ½åœ¨åº”ç”¨æ ¹ç›®å½•ä¸‹ã€‚")
        return None, None, None, None, None
    except Exception as e:
        st.error(f"åŠ è½½èµ„æºæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return None, None, None, None, None

@st.cache_data
def load_pareto_data():
    """åŠ è½½å¸•ç´¯æ‰˜è§£é›†æ•°æ®"""
    try:
        df = pd.read_csv("å¸•ç´¯æ‰˜è§£é›†.csv")
        return df
    except FileNotFoundError as e:
        st.error(f"åŠ è½½æ•°æ®æ–‡ä»¶æ—¶å‡ºé”™: {e}ã€‚è¯·ç¡®ä¿ 'å¸•ç´¯æ‰˜è§£é›†.csv' æ–‡ä»¶å­˜åœ¨ã€‚")
        return pd.DataFrame()

# åŠ è½½æ‰€æœ‰èµ„æº
udi_model, dgi_model, ceui_model, scaler, device = load_models_and_scaler()
df_solutions = load_pareto_data()

# --- 2. åœºæ™¯ä¸€ï¼šå¿«é€Ÿæ€§èƒ½é¢„æµ‹é¡µé¢ ---

def prediction_page():
    st.header("åœºæ™¯ä¸€ï¼šå¿«é€Ÿæ€§èƒ½é¢„æµ‹")
    st.markdown("é€šè¿‡æ»‘å—è¾“å…¥å•ä¸ªè®¾è®¡æ–¹æ¡ˆçš„å‚æ•°ï¼Œç³»ç»Ÿå°†è°ƒç”¨é¢„æµ‹æ¨¡å‹è®¡ç®—å…¶æ€§èƒ½ã€‚")
    
    feature_names = ['å¼€é—´', 'è¿›æ·±', 'å±‚é«˜', 'åŒ—ä¾§çª—å°é«˜', 'åŒ—ä¾§çª—é«˜', 'åŒ—ä¾§çª—å¢™æ¯”', 'å—ä¾§çª—å¢™æ¯”', 'å—ä¾§çª—é«˜', 'å—ä¾§çª—å°é«˜', 'å—ä¾§çª—é—´è·']
    input_vars = {}
    
    st.info("è¯·æ‹–åŠ¨ä¸‹é¢çš„æ»‘å—æ¥è®¾ç½®è®¾è®¡å‚æ•°å€¼:")
    cols = st.columns(5)
    # è®¾å®šä¸€äº›æ›´åˆç†çš„é»˜è®¤å€¼å’ŒèŒƒå›´
    default_values = {'å¼€é—´': 7.2, 'è¿›æ·±': 9.0, 'å±‚é«˜': 3.6, 'åŒ—ä¾§çª—å°é«˜': 0.9, 'åŒ—ä¾§çª—é«˜': 1.8, 'åŒ—ä¾§çª—å¢™æ¯”': 0.3, 'å—ä¾§çª—å¢™æ¯”': 0.5, 'å—ä¾§çª—é«˜': 1.8, 'å—ä¾§çª—å°é«˜': 0.9, 'å—ä¾§çª—é—´è·': 1.2}
    min_max_step = {'å¼€é—´': (3.0, 10.0, 0.1), 'è¿›æ·±': (5.0, 12.0, 0.1), 'å±‚é«˜': (2.8, 5.0, 0.1), 'åŒ—ä¾§çª—å°é«˜': (0.5, 1.5, 0.1), 'åŒ—ä¾§çª—é«˜': (1.0, 3.0, 0.1), 'åŒ—ä¾§çª—å¢™æ¯”': (0.1, 0.8, 0.05), 'å—ä¾§çª—å¢™æ¯”': (0.1, 0.8, 0.05), 'å—ä¾§çª—é«˜': (1.0, 3.0, 0.1), 'å—ä¾§çª—å°é«˜': (0.5, 1.5, 0.1), 'å—ä¾§çª—é—´è·': (0.5, 3.0, 0.1)}

    for i, name in enumerate(feature_names):
        with cols[i % 5]:
            min_val, max_val, step_val = min_max_step[name]
            input_vars[name] = st.slider(f'{name}', min_value=min_val, max_value=max_val, value=default_values[name], step=step_val)

    if st.button("å¼€å§‹é¢„æµ‹", key="predict_button", use_container_width=True):
        if not all([udi_model, dgi_model, ceui_model, scaler, device]):
            st.error("æ¨¡å‹æˆ–ScaleræœªæˆåŠŸåŠ è½½ï¼Œæ— æ³•è¿›è¡Œé¢„æµ‹ã€‚è¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å¹¶åˆ·æ–°é¡µé¢ã€‚")
            return

        with st.spinner('æ­£åœ¨è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†å’Œæ¨¡å‹é¢„æµ‹...'):
            try:
                # 1. æ•´ç†è¾“å…¥æ•°æ®å¹¶ç¡®ä¿é¡ºåºæ­£ç¡®
                input_data = np.array([[input_vars[name] for name in feature_names]])
                
                # 2. ä½¿ç”¨åŠ è½½çš„scalerè¿›è¡Œæ ‡å‡†åŒ–
                scaled_data = scaler.transform(input_data)
                
                # 3. è½¬æ¢ä¸ºPyTorchå¼ é‡ï¼Œå¹¶ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
                input_tensor = torch.FloatTensor(scaled_data).to(device)

                # 4. è°ƒç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
                with torch.no_grad():
                    pred_udi = udi_model(input_tensor).item()
                    pred_dgi = dgi_model(input_tensor).item()
                    pred_ceui = ceui_model(input_tensor).item()
                
                st.subheader("ğŸš€ æ€§èƒ½é¢„æµ‹ç»“æœ:")
                c1, c2, c3 = st.columns(3)
                c1.metric("æœ‰æ•ˆé‡‡å…‰ç…§åº¦ (UDI)", f"{pred_udi:.2f} %")
                c2.metric("ä¸èˆ’é€‚çœ©å…‰æŒ‡æ•° (DGI)", f"{pred_dgi:.2f}")
                c3.metric("åˆ¶å†·èƒ½è€—å¼ºåº¦ (CEUI)", f"{pred_ceui:.2f} kWh/mÂ²")

            except Exception as e:
                st.error(f"é¢„æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

# --- ç»Ÿä¸€AI APIè°ƒç”¨å‡½æ•° ---
def call_ai_api(user_preference, provider, api_key):
    """ç»Ÿä¸€çš„AI APIè°ƒç”¨å‡½æ•°ï¼Œæ”¯æŒå¤šä¸ªæä¾›å•†"""
    if not AI_AVAILABLE:
        return {
            "weights": {"UDI": 0.33, "CEUI": 0.33, "DGI": 0.34},
            "reasoning": f"{provider} APIæœªé…ç½®ï¼Œä½¿ç”¨é»˜è®¤ç­‰æƒé‡åˆ†é…ã€‚"
        }
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä½é¡¶å°–çš„å»ºç­‘è®¾è®¡ä¸æ€§èƒ½ä¼˜åŒ–ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æç”¨æˆ·çš„è‡ªç„¶è¯­è¨€è®¾è®¡åå¥½ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºä¸€ä¸ªåŒ…å«'weights'å’Œ'reasoning'çš„JSONå¯¹è±¡ã€‚
    
    # æŒ‡æ ‡å«ä¹‰è¯´æ˜ï¼š
    - UDI (æœ‰æ•ˆé‡‡å…‰ç…§åº¦): ä»£è¡¨å®¤å†…"é‡‡å…‰"æ°´å¹³ï¼Œæ•°å€¼è¶Šé«˜è¶Šå¥½ã€‚
    - CEUI (åˆ¶å†·èƒ½è€—å¼ºåº¦): ä»£è¡¨å»ºç­‘"èƒ½è€—"ï¼Œæ•°å€¼è¶Šä½è¶Šå¥½ã€‚
    - DGI (ä¸èˆ’é€‚çœ©å…‰æŒ‡æ•°): ä»£è¡¨å®¤å†…"çœ©å…‰"ç¨‹åº¦ï¼Œæ•°å€¼è¶Šä½è¶Šå¥½ã€‚

    # ä½ çš„ä»»åŠ¡ï¼š
    1. ä»”ç»†é˜…è¯»ç”¨æˆ·çš„åå¥½æè¿°: '{user_preference}'
    2. **ä¸è¦ä½¿ç”¨ä»»ä½•ç¡¬æ€§ç­›é€‰æ¡ä»¶ã€‚**
    3. ç”Ÿæˆä¸€ä¸ª'weights'å­—å…¸ï¼Œè¡¨è¾¾ä¸‰ä¸ªæŒ‡æ ‡çš„ç›¸å¯¹é‡è¦æ€§ã€‚æƒé‡è¶Šé«˜çš„æŒ‡æ ‡ï¼Œä»£è¡¨ç”¨æˆ·è¶Šå…³å¿ƒã€‚ä¸‰ä¸ªæƒé‡çš„æ€»å’Œå¿…é¡»ä¸º1.0ã€‚
    4. ç”Ÿæˆä¸€ä¸ª'reasoning'å­—ç¬¦ä¸²ï¼Œç”¨ä¸€ä¸¤å¥è¯ç®€è¦è§£é‡Šä½ ä¸ºä»€ä¹ˆè¿™æ ·åˆ†é…æƒé‡ã€‚

    # ç¤ºä¾‹ï¼š
    å¦‚æœç”¨æˆ·è¯´ï¼š"æˆ‘æœ€å…³å¿ƒçš„æ˜¯èŠ‚èƒ½ï¼Œçœ©å…‰å…¶æ¬¡ï¼Œé‡‡å…‰å·®ä¸å¤šå°±è¡Œã€‚"
    ä½ çš„è¾“å‡ºåº”è¯¥æ˜¯ç±»ä¼¼è¿™æ ·çš„JSON:
    {{
        "weights": {{
            "UDI": 0.2,
            "CEUI": 0.5,
            "DGI": 0.3
        }},
        "reasoning": "æ ¹æ®ç”¨æˆ·çš„æè¿°ï¼ŒèŠ‚èƒ½(CEUI)æ˜¯é¦–è¦ç›®æ ‡ï¼Œå› æ­¤åˆ†é…äº†æœ€é«˜æƒé‡ã€‚çœ©å…‰(DGI)ä¸ºæ¬¡è¦ç›®æ ‡ï¼Œé‡‡å…‰(UDI)è¦æ±‚ä¸é«˜ï¼Œæƒé‡æœ€ä½ã€‚"
    }}
    
    è¯·ç›´æ¥è¿”å›JSONæ ¼å¼çš„ç»“æœï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ã€‚
    """
    
    try:
        if provider == "OpenAI":
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½å»ºç­‘è®¾è®¡ä¸æ€§èƒ½ä¼˜åŒ–ä¸“å®¶ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=500
            )
            result_text = response.choices[0].message.content.strip()
            
        elif provider == "Google Gemini":
            model = genai.GenerativeModel('gemini-pro')
            response = model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.7,
                    max_output_tokens=500,
                )
            )
            result_text = response.text.strip()
            
        elif provider == "DeepSeek":
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }
            data = {
                "model": "deepseek-chat",
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½å»ºç­‘è®¾è®¡ä¸æ€§èƒ½ä¼˜åŒ–ä¸“å®¶ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.7,
                "max_tokens": 500
            }
            response = requests.post(
                "https://api.deepseek.com/v1/chat/completions",
                headers=headers,
                json=data
            )
            if response.status_code == 200:
                result_text = response.json()["choices"][0]["message"]["content"].strip()
            else:
                raise Exception(f"DeepSeek APIè°ƒç”¨å¤±è´¥: {response.status_code}")
        
        # å°è¯•è§£æJSON
        try:
            result = json.loads(result_text)
            return result
        except json.JSONDecodeError:
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤æƒé‡
            return {
                "weights": {"UDI": 0.33, "CEUI": 0.33, "DGI": 0.34},
                "reasoning": "APIè¿”å›æ ¼å¼è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­‰æƒé‡åˆ†é…ã€‚"
            }
            
    except Exception as e:
        st.error(f"{provider} APIè°ƒç”¨å¤±è´¥: {e}")
        return {
            "weights": {"UDI": 0.33, "CEUI": 0.33, "DGI": 0.34},
            "reasoning": "APIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­‰æƒé‡åˆ†é…ã€‚"
        }

# --- 3. åœºæ™¯äºŒï¼šå¤šæ–¹æ¡ˆæ™ºèƒ½å†³ç­–é¡µé¢ (æ”¯æŒå¤šAIæä¾›å•†) ---
def decision_page():
    st.header("åœºæ™¯äºŒï¼šå¤šæ–¹æ¡ˆæ™ºèƒ½å†³ç­–")
    st.markdown("ç”¨è‡ªç„¶è¯­è¨€æè¿°æ‚¨çš„è®¾è®¡åå¥½ï¼ŒAIå°†ä¸ºæ‚¨ä»å¸•ç´¯æ‰˜æœ€ä¼˜æ–¹æ¡ˆä¸­æ™ºèƒ½æ¨èã€‚")
    
    # é¢„è®¾çš„å¸•ç´¯æ‰˜è§£é›†åˆ—åä¸ç”¨æˆ·å‹å¥½åç§°çš„æ˜ å°„
    feature_names = ['å¼€é—´', 'è¿›æ·±', 'å±‚é«˜', 'åŒ—ä¾§çª—å°é«˜', 'åŒ—ä¾§çª—é«˜', 'åŒ—ä¾§çª—å¢™æ¯”', 'å—ä¾§çª—å¢™æ¯”', 'å—ä¾§çª—é«˜', 'å—ä¾§çª—å°é«˜', 'å—ä¾§çª—é—´è·']

    user_preference = st.text_area(
        "è¯·è¾“å…¥æ‚¨çš„è®¾è®¡åå¥½:",
        placeholder="ä¾‹å¦‚ï¼šæˆ‘å¸Œæœ›æ•™å®¤èƒ½å°½å¯èƒ½åœ°èŠ‚èƒ½ï¼ŒåŒæ—¶ä¸¥æ ¼æ§åˆ¶çœ©å…‰ï¼Œé‡‡å…‰è¾¾åˆ°è‰¯å¥½æ°´å¹³å³å¯ã€‚",
        height=150
    )

    if st.button("å¼€å§‹æ™ºèƒ½æ¨è", key="decision_button", use_container_width=True):
        if df_solutions.empty:
            st.error("å¸•ç´¯æ‰˜è§£é›†æ•°æ® ('å¸•ç´¯æ‰˜è§£é›†.csv') æœªæˆåŠŸåŠ è½½ï¼Œæ— æ³•è¿›è¡Œå†³ç­–ã€‚")
            return
        if not user_preference:
            st.warning("è¯·è¾“å…¥æ‚¨çš„è®¾è®¡åå¥½ï¼")
            return

        with st.spinner(f'{ai_provider} AIä¸“å®¶æ­£åœ¨åˆ†ææ‚¨çš„åå¥½å¹¶è¿›è¡Œå†³ç­–...'):
            try:
                # è°ƒç”¨ç»Ÿä¸€AI APIè·å–å†³ç­–ç­–ç•¥
                decision_strategy = call_ai_api(user_preference, ai_provider, api_key)
                
                st.subheader("AIä¸“å®¶å¯¹æ‚¨åå¥½çš„è§£è¯»:")
                st.info(f"æ¨èç†ç”±: {decision_strategy.get('reasoning', 'æ— ')}")
                st.json({"weights": decision_strategy.get('weights')}) # åªæ˜¾ç¤ºæƒé‡éƒ¨åˆ†

                df_processed = df_solutions.copy()

                # --- æ ¸å¿ƒæ’åºé€»è¾‘ (å¯¹æ‰€æœ‰æ–¹æ¡ˆè¿›è¡Œæ“ä½œ) ---
                
                # 1. å½’ä¸€åŒ– (Normalization)
                # DGIå’ŒCEUIæ˜¯æˆæœ¬å‹æŒ‡æ ‡ï¼ˆè¶Šå°è¶Šå¥½ï¼‰ï¼ŒUDIæ˜¯æ•ˆç›Šå‹æŒ‡æ ‡ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
                # æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å°†æ‰€æœ‰æŒ‡æ ‡éƒ½è½¬æ¢ä¸º"è¶Šå¤§è¶Šå¥½"çš„å¾—åˆ†
                for col in ['UDI', 'DGI', 'CEUI']:
                    min_val, max_val = df_processed[col].min(), df_processed[col].max()
                    if (max_val - min_val) > 0:
                        if col == 'UDI': # æ•ˆç›Šå‹æŒ‡æ ‡ï¼Œç›´æ¥å½’ä¸€åŒ–
                            df_processed[f'{col}_norm'] = (df_processed[col] - min_val) / (max_val - min_val)
                        else: # æˆæœ¬å‹æŒ‡æ ‡ (DGI, CEUI)ï¼Œåå‘å½’ä¸€åŒ–
                            df_processed[f'{col}_norm'] = (max_val - df_processed[col]) / (max_val - min_val)
                    else:
                        # å¦‚æœä¸€ä¸ªæŒ‡æ ‡åœ¨æ‰€æœ‰æ–¹æ¡ˆä¸­éƒ½ä¸€æ ·ï¼Œé‚£ä¹ˆå®ƒçš„å½’ä¸€åŒ–å¾—åˆ†ä¸º1ï¼ˆæˆ–0.5ï¼‰ï¼Œä¸å½±å“æ’åº
                        df_processed[f'{col}_norm'] = 1.0

                # 2. åŠ æƒè¯„åˆ† (Weighted Scoring)
                weights = decision_strategy.get('weights', {})
                w_udi = weights.get('UDI', 1/3) # å¦‚æœAPIæ²¡ç»™ï¼Œå°±ç”¨é»˜è®¤ç­‰æƒé‡
                w_dgi = weights.get('DGI', 1/3)
                w_ceui = weights.get('CEUI', 1/3)
                
                # æ‰€æœ‰å½’ä¸€åŒ–åçš„æŒ‡æ ‡éƒ½æ˜¯"è¶Šå¤§è¶Šå¥½"ï¼Œæ‰€ä»¥ç›´æ¥åŠ æƒæ±‚å’Œ
                df_processed['score'] = (
                    df_processed['UDI_norm'] * w_udi + 
                    df_processed['DGI_norm'] * w_dgi + 
                    df_processed['CEUI_norm'] * w_ceui
                )
                
                # 3. æ’åºå’Œå±•ç¤º (score è¶Šå¤§è¶Šå¥½)
                final_recommendations = df_processed.sort_values(by='score', ascending=False).head(5)
                
                st.subheader("ä¸ºæ‚¨æ¨èçš„æœ€ä½³5ä¸ªæ–¹æ¡ˆ:")
                # ç¡®ä¿åªå±•ç¤ºæ•°æ®é›†ä¸­å­˜åœ¨çš„åˆ—
                display_cols = ['UDI', 'DGI', 'CEUI', 'score'] + [col for col in feature_names if col in final_recommendations.columns]
                st.dataframe(final_recommendations[display_cols].style.format("{:.2f}"))

            except Exception as e:
                st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                st.exception(e) # æ‰“å°è¯¦ç»†çš„é”™è¯¯å †æ ˆï¼Œæ–¹ä¾¿è°ƒè¯•

# --- 4. ä¸»ç¨‹åºä¸å¯¼èˆª ---
def main():
    st.sidebar.title("åŠŸèƒ½å¯¼èˆª")
    
    # æ£€æŸ¥èµ„æºæ˜¯å¦åŠ è½½æˆåŠŸ
    if not all([udi_model, dgi_model, ceui_model, scaler, device]):
        st.sidebar.error("æ ¸å¿ƒèµ„æºåŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æˆ–åˆ·æ–°ã€‚")
        st.header("ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
        st.write("æ— æ³•åŠ è½½å¿…è¦çš„æ¨¡å‹æˆ–æ•°æ®æ–‡ä»¶ï¼Œè¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶ä¸`app.py`åœ¨åŒä¸€ç›®å½•ä¸‹ï¼Œå¹¶æ£€æŸ¥ç»ˆç«¯é”™è¯¯ä¿¡æ¯ï¼š")
        st.code("""
- UDI_trained_model.pth
- DGI_trained_model.pth
- CEUI_trained_model.pth
- scaler.pkl
- å¸•ç´¯æ‰˜è§£é›†.csv
        """)
        return

    page_options = ["å¿«é€Ÿæ€§èƒ½é¢„æµ‹", "å¤šæ–¹æ¡ˆæ™ºèƒ½å†³ç­–"]
    page = st.sidebar.radio("è¯·é€‰æ‹©ä¸€ä¸ªåŠŸèƒ½åœºæ™¯", page_options)

    if page == "å¿«é€Ÿæ€§èƒ½é¢„æµ‹":
        prediction_page()
    elif page == "å¤šæ–¹æ¡ˆæ™ºèƒ½å†³ç­–":
        decision_page()

if __name__ == "__main__":
    main()